{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simon/PycharmProjects/receiptlayoutlm/code/../hugging_face_repo is already a clone of https://huggingface.co/sibrun/receiptlayoutlm. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "03/12/2022 22:20:48 - WARNING - huggingface_hub.repository - /Users/simon/PycharmProjects/receiptlayoutlm/code/../hugging_face_repo is already a clone of https://huggingface.co/sibrun/receiptlayoutlm. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from huggingface_hub import Repository"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hf_repo = Repository(\"../hugging_face_repo\", clone_from=\"https://huggingface.co/sibrun/receiptlayoutlm\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "hf_repo = Repository(\"../hugging_face_repo\")\n",
    "hf_repo.git_pull()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "{'image': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\n 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n 'bbox': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),\n 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "ds_receipts = load_from_disk(\"../datasets/ds_receipts_final\")\n",
    "#ds_receipts = load_dataset(\"sibrun/receipts\", use_auth_token=True)\n",
    "ds_receipts['train'].features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "from datasets import Features, Array2D, Array3D, Sequence, Value\n",
    "max_length = 512\n",
    "features = Features({\n",
    "    'image': Array3D(dtype=\"uint8\", shape=(3, 224, 224)),\n",
    "    'input_ids': Sequence(feature=Value(dtype=\"int32\"), length=max_length),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(max_length, 4)),\n",
    "    'labels': Sequence(feature=Value(dtype=\"int64\"), length=max_length),\n",
    "    'attention_mask': Sequence(feature=Value(dtype=\"int8\"), length=max_length),\n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/12/2022 22:02:20 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at ../datasets/ds_receipts_final/train/cache-e5c271fb43f54ac8.arrow\n",
      "03/12/2022 22:02:20 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at ../datasets/ds_receipts_final/test/cache-c1bc42bb40029ecf.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'image': Array3D(shape=(3, 224, 224), dtype='uint8', id=None),\n 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=512, id=None),\n 'bbox': Array2D(shape=(512, 4), dtype='int64', id=None),\n 'labels': Sequence(feature=Value(dtype='int64', id=None), length=512, id=None),\n 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=512, id=None)}"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_receipts = ds_receipts.cast(features)\n",
    "ds_receipts['train'].features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "label_names = ['company', 'date', 'address', 'total']\n",
    "labels = ['O'] + label_names\n",
    "num_labels = len(labels)\n",
    "ids_to_labels = {k: v for k, v in enumerate(labels)}\n",
    "labels_to_ids = {v: k for k, v in enumerate(labels)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "xlm_config = AutoConfig.from_pretrained(\"microsoft/layoutxlm-base\",\n",
    "                                         num_labels=num_labels,\n",
    "                                         id2label=ids_to_labels,\n",
    "                                         label2id=labels_to_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutxlm-base were not used when initializing LayoutLMv2ForTokenClassification: ['layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked']\n",
      "- This IS expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMv2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LayoutLMv2ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutxlm-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv2ForTokenClassification\n",
    "\n",
    "model_xlm = LayoutLMv2ForTokenClassification.from_pretrained(\"microsoft/layoutxlm-base\", config=xlm_config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "model_xlm.save_pretrained(\"../models/token_class_xlm_base\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "model_xlm = LayoutLMv2ForTokenClassification.from_pretrained(\"../models/token_class_xlm_base\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "LayoutLMv2ForTokenClassification(\n  (layoutlmv2): LayoutLMv2Model(\n    (embeddings): LayoutLMv2Embeddings(\n      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768)\n      (x_position_embeddings): Embedding(1024, 128)\n      (y_position_embeddings): Embedding(1024, 128)\n      (h_position_embeddings): Embedding(1024, 128)\n      (w_position_embeddings): Embedding(1024, 128)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (visual): LayoutLMv2VisualBackbone(\n      (backbone): FPN(\n        (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (top_block): LastLevelMaxPool()\n        (bottom_up): ResNet(\n          (stem): BasicStem(\n            (conv1): Conv2d(\n              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n            )\n          )\n          (res2): Sequential(\n            (0): BottleneckBlock(\n              (shortcut): Conv2d(\n                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv1): Conv2d(\n                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n            )\n            (1): BottleneckBlock(\n              (conv1): Conv2d(\n                256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n            )\n            (2): BottleneckBlock(\n              (conv1): Conv2d(\n                256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n            )\n          )\n          (res3): Sequential(\n            (0): BottleneckBlock(\n              (shortcut): Conv2d(\n                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv1): Conv2d(\n                256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n            )\n            (1): BottleneckBlock(\n              (conv1): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n            )\n            (2): BottleneckBlock(\n              (conv1): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n            )\n            (3): BottleneckBlock(\n              (conv1): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n            )\n          )\n          (res4): Sequential(\n            (0): BottleneckBlock(\n              (shortcut): Conv2d(\n                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv1): Conv2d(\n                512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (1): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (2): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (3): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (4): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (5): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (6): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (7): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (8): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (9): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (10): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (11): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (12): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (13): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (14): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (15): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (16): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (17): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (18): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (19): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (20): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (21): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (22): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n          )\n          (res5): Sequential(\n            (0): BottleneckBlock(\n              (shortcut): Conv2d(\n                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv1): Conv2d(\n                1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n            )\n            (1): BottleneckBlock(\n              (conv1): Conv2d(\n                2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n            )\n            (2): BottleneckBlock(\n              (conv1): Conv2d(\n                2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n            )\n          )\n        )\n      )\n      (pool): AdaptiveAvgPool2d(output_size=[7, 7])\n    )\n    (visual_proj): Linear(in_features=256, out_features=768, bias=True)\n    (visual_LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (visual_dropout): Dropout(p=0.1, inplace=False)\n    (encoder): LayoutLMv2Encoder(\n      (layer): ModuleList(\n        (0): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): LayoutLMv2Pooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ds_receipts.set_format(type=\"torch\", device=device)\n",
    "model_xlm.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# Commented parameters correspond to the small model\n",
    "config = {\"train_batch_size\": 4,\n",
    "          \"valid_batch_size\": 2,\n",
    "          \"weight_decay\": 0.1,\n",
    "          \"learning_rate\": 5e-5,\n",
    "          \"num_train_epochs\": 2,\n",
    "          \"seed\": 1,\n",
    "          \"save_checkpoint_steps\": 100}\n",
    "\n",
    "args = Namespace(**config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(ds_receipts['train'], batch_size=args.train_batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(ds_receipts['test'], batch_size=args.valid_batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image torch.Size([4, 3, 224, 224])\n",
      "input_ids torch.Size([4, 512])\n",
      "bbox torch.Size([4, 512, 4])\n",
      "labels torch.Size([4, 512])\n",
      "attention_mask torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(k, v.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model_xlm.parameters(), lr=args.learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "import wandb\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "logging_file_path = \"../log/train.log\"\n",
    "wandb_dir = \"../log/wandb\"\n",
    "project_name = \"receiptlayoutlm\"\n",
    "\n",
    "def setup_logger():\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "        handlers=[logging.FileHandler(logging_file_path), logging.StreamHandler()])\n",
    "    #wandb.init(project=project_name_wandb,\n",
    "    #           config=args,\n",
    "    #           dir=wandb_dir,\n",
    "    #           settings=wandb.Settings(start_method=\"fork\"),\n",
    "    #           entity=\"sibrun\",\n",
    "    #           sync_tensorboard=True)\n",
    "    #run_name = wandb.run.name\n",
    "    #tb_writer = SummaryWriter()\n",
    "    #tb_writer.add_hparams(vars(args), {'0': 0})\n",
    "    logger.setLevel(logging.INFO)\n",
    "    #datasets.utils.logging.set_verbosity_debug()\n",
    "    #transformers.utils.logging.set_verbosity_info()\n",
    "    return logger  #, tb_writer , run_name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "def log_metrics(step, metrics):\n",
    "    logger.info(f\"Step {step}: {metrics}\")\n",
    "    #wandb.log(metrics)\n",
    "    #[tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model_xlm.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            model_outputs = model_xlm(**batch)\n",
    "        losses.append(model_outputs.loss)\n",
    "    total_loss = torch.mean(torch.stack(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(total_loss)\n",
    "    except OverflowError:\n",
    "            perplexity = torch.tensor(float(\"inf\"))\n",
    "    return loss.item(), perplexity.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "from random import seed\n",
    "seed(args.seed)\n",
    "\n",
    "logger = setup_logger()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simon/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py:772: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(\n",
      "/Users/simon/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py:782: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(\n",
      "03/12/2022 22:19:34 - INFO - __main__ - Step 0: {'steps': 0, 'loss/train': 1.2323269844055176}\n",
      "03/12/2022 22:19:34 - INFO - __main__ - Evaluating and saving model checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0 steps: 1.2323269844055176\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [96]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m step \u001B[38;5;241m%\u001B[39m args\u001B[38;5;241m.\u001B[39msave_checkpoint_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     13\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEvaluating and saving model checkpoint\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 14\u001B[0m     eval_loss, perplexity \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m     log_metrics(step, {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss/eval\u001B[39m\u001B[38;5;124m'\u001B[39m: eval_loss, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mperplexity\u001B[39m\u001B[38;5;124m'\u001B[39m: perplexity})\n\u001B[1;32m     16\u001B[0m     model_xlm\u001B[38;5;241m.\u001B[39msave_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../models/receiptlayoutlm\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Input \u001B[0;32mIn [94]\u001B[0m, in \u001B[0;36mevaluate\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(eval_dataloader):\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m----> 6\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_xlm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(model_outputs\u001B[38;5;241m.\u001B[39mloss)\n\u001B[1;32m      8\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmean(torch\u001B[38;5;241m.\u001B[39mstack(losses))\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py:1181\u001B[0m, in \u001B[0;36mLayoutLMv2ForTokenClassification.forward\u001B[0;34m(self, input_ids, bbox, image, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1152\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1153\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1154\u001B[0m \u001B[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1176\u001B[0m \u001B[38;5;124;03m>>> logits = outputs.logits\u001B[39;00m\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;124;03m```\"\"\"\u001B[39;00m\n\u001B[1;32m   1179\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1181\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayoutlmv2\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1182\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1183\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbbox\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbbox\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1185\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1186\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1187\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1188\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1189\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1190\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1192\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1193\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1195\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39msize()\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py:897\u001B[0m, in \u001B[0;36mLayoutLMv2Model.forward\u001B[0;34m(self, input_ids, bbox, image, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    887\u001B[0m     bbox \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mlist\u001B[39m(input_shape) \u001B[38;5;241m+\u001B[39m [\u001B[38;5;241m4\u001B[39m]), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m    889\u001B[0m text_layout_emb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_calc_text_embeddings(\n\u001B[1;32m    890\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    891\u001B[0m     bbox\u001B[38;5;241m=\u001B[39mbbox,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    894\u001B[0m     inputs_embeds\u001B[38;5;241m=\u001B[39minputs_embeds,\n\u001B[1;32m    895\u001B[0m )\n\u001B[0;32m--> 897\u001B[0m visual_emb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_calc_img_embeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    898\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    899\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbbox\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvisual_bbox\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    900\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvisual_position_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    901\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    902\u001B[0m final_emb \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([text_layout_emb, visual_emb], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    904\u001B[0m extended_attention_mask \u001B[38;5;241m=\u001B[39m final_attention_mask\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py:760\u001B[0m, in \u001B[0;36mLayoutLMv2Model._calc_img_embeddings\u001B[0;34m(self, image, bbox, position_ids)\u001B[0m\n\u001B[1;32m    759\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_calc_img_embeddings\u001B[39m(\u001B[38;5;28mself\u001B[39m, image, bbox, position_ids):\n\u001B[0;32m--> 760\u001B[0m     visual_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvisual_proj(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvisual\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    761\u001B[0m     position_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings\u001B[38;5;241m.\u001B[39mposition_embeddings(position_ids)\n\u001B[1;32m    762\u001B[0m     spatial_position_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings\u001B[38;5;241m.\u001B[39m_calc_spatial_position_embeddings(bbox)\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py:589\u001B[0m, in \u001B[0;36mLayoutLMv2VisualBackbone.forward\u001B[0;34m(self, images)\u001B[0m\n\u001B[1;32m    587\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, images):\n\u001B[1;32m    588\u001B[0m     images_input \u001B[38;5;241m=\u001B[39m ((images \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mis_tensor(images) \u001B[38;5;28;01melse\u001B[39;00m images\u001B[38;5;241m.\u001B[39mtensor) \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpixel_mean) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpixel_std\n\u001B[0;32m--> 589\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackbone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    590\u001B[0m     features \u001B[38;5;241m=\u001B[39m features[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_feature_key]\n\u001B[1;32m    591\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(features)\u001B[38;5;241m.\u001B[39mflatten(start_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/detectron2/modeling/backbone/fpn.py:126\u001B[0m, in \u001B[0;36mFPN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03m        input (dict[str->Tensor]): mapping feature map name (e.g., \"res5\") to\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;124;03m            [\"p2\", \"p3\", ..., \"p6\"].\u001B[39;00m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 126\u001B[0m     bottom_up_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbottom_up\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m     results \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    128\u001B[0m     prev_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlateral_convs[\u001B[38;5;241m0\u001B[39m](bottom_up_features[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_features[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]])\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/detectron2/modeling/backbone/resnet.py:449\u001B[0m, in \u001B[0;36mResNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    447\u001B[0m     outputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstem\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m x\n\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, stage \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstage_names, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstages):\n\u001B[0;32m--> 449\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mstage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    450\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_out_features:\n\u001B[1;32m    451\u001B[0m         outputs[name] \u001B[38;5;241m=\u001B[39m x\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 141\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/detectron2/modeling/backbone/resnet.py:201\u001B[0m, in \u001B[0;36mBottleneckBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    198\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(out)\n\u001B[1;32m    199\u001B[0m out \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu_(out)\n\u001B[0;32m--> 201\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv3\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshortcut \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     shortcut \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshortcut(x)\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/detectron2/layers/wrappers.py:106\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[1;32m    101\u001B[0m         \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/issues/12013\u001B[39;00m\n\u001B[1;32m    102\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m    103\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm, torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mSyncBatchNorm\n\u001B[1;32m    104\u001B[0m         ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSyncBatchNorm does not support empty inputs!\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 106\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    110\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "model_xlm.train()\n",
    "for epoch in range(args.num_train_epochs):\n",
    "   print(\"Epoch:\", epoch)\n",
    "   for batch in train_dataloader:\n",
    "       optimizer.zero_grad()\n",
    "       outputs = model_xlm(**batch)\n",
    "       loss = outputs.loss\n",
    "       log_metrics(step, {'steps': step, 'loss/train': loss.item()})\n",
    "       if step % 10 == 0:\n",
    "           print(f\"Loss after {step} steps: {loss.item()}\")\n",
    "       if step % args.save_checkpoint_steps == 0:\n",
    "           logger.info('Evaluating and saving model checkpoint')\n",
    "           eval_loss, perplexity = evaluate()\n",
    "           log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n",
    "           model_xlm.save_pretrained(\"../models/receiptlayoutlm\")\n",
    "           model_xlm.push_to_hub(\"sibrun/receiptlayoutlm\", commit_message=f'step {step}')\n",
    "       model_xlm.train()\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "       step += 1\n",
    "\n",
    "logger.info('Evaluating and saving model after training')\n",
    "eval_loss, perplexity = evaluate()\n",
    "log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n",
    "model_xlm.save_pretrained(\"../models/receiptlayoutlm\")\n",
    "model_xlm.push_to_hub(\"sibrun/receiptlayoutlm\", commit_message='final model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}