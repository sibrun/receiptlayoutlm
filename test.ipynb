{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_names = ['company', 'date', 'address', 'total']\n",
    "labels = ['O'] + label_names\n",
    "num_labels = len(labels)\n",
    "ids_to_labels = {k: v for k, v in enumerate(labels)}\n",
    "labels_to_ids = {v: k for k, v in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LayoutLMv2Tokenizer'. \n",
      "The class this function is called from is 'LayoutXLMTokenizer'.\n"
     ]
    },
    {
     "data": {
      "text/plain": "LayoutLMv2ForTokenClassification(\n  (layoutlmv2): LayoutLMv2Model(\n    (embeddings): LayoutLMv2Embeddings(\n      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768)\n      (x_position_embeddings): Embedding(1024, 128)\n      (y_position_embeddings): Embedding(1024, 128)\n      (h_position_embeddings): Embedding(1024, 128)\n      (w_position_embeddings): Embedding(1024, 128)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (visual): LayoutLMv2VisualBackbone(\n      (backbone): FPN(\n        (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (top_block): LastLevelMaxPool()\n        (bottom_up): ResNet(\n          (stem): BasicStem(\n            (conv1): Conv2d(\n              3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n              (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n            )\n          )\n          (res2): Sequential(\n            (0): BottleneckBlock(\n              (shortcut): Conv2d(\n                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv1): Conv2d(\n                64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n            )\n            (1): BottleneckBlock(\n              (conv1): Conv2d(\n                256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n            )\n            (2): BottleneckBlock(\n              (conv1): Conv2d(\n                256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n              )\n            )\n          )\n          (res3): Sequential(\n            (0): BottleneckBlock(\n              (shortcut): Conv2d(\n                256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv1): Conv2d(\n                256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n            )\n            (1): BottleneckBlock(\n              (conv1): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n            )\n            (2): BottleneckBlock(\n              (conv1): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n            )\n            (3): BottleneckBlock(\n              (conv1): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n              )\n            )\n          )\n          (res4): Sequential(\n            (0): BottleneckBlock(\n              (shortcut): Conv2d(\n                512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv1): Conv2d(\n                512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (1): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (2): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (3): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (4): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (5): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (6): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (7): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (8): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (9): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (10): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (11): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (12): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (13): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (14): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (15): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (16): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (17): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (18): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (19): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (20): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (21): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n            (22): BottleneckBlock(\n              (conv1): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n              )\n            )\n          )\n          (res5): Sequential(\n            (0): BottleneckBlock(\n              (shortcut): Conv2d(\n                1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv1): Conv2d(\n                1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n            )\n            (1): BottleneckBlock(\n              (conv1): Conv2d(\n                2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n            )\n            (2): BottleneckBlock(\n              (conv1): Conv2d(\n                2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv2): Conv2d(\n                2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n              (conv3): Conv2d(\n                2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n                (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n              )\n            )\n          )\n        )\n      )\n      (pool): AdaptiveAvgPool2d(output_size=[7, 7])\n    )\n    (visual_proj): Linear(in_features=256, out_features=768, bias=True)\n    (visual_LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (visual_dropout): Dropout(p=0.1, inplace=False)\n    (encoder): LayoutLMv2Encoder(\n      (layer): ModuleList(\n        (0): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): LayoutLMv2Layer(\n          (attention): LayoutLMv2Attention(\n            (self): LayoutLMv2SelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): LayoutLMv2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): LayoutLMv2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): LayoutLMv2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): LayoutLMv2Pooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=5, bias=True)\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LayoutLMv2FeatureExtractor, LayoutXLMTokenizer, LayoutXLMProcessor\n",
    "from transformers import LayoutLMv2ForTokenClassification\n",
    "\n",
    "feature_extractor_xlm = LayoutLMv2FeatureExtractor()\n",
    "tokenizer_xlm = LayoutXLMTokenizer.from_pretrained(\"microsoft/layoutxlm-base\")\n",
    "processor_xlm = LayoutXLMProcessor(feature_extractor_xlm, tokenizer_xlm)\n",
    "\n",
    "model_xlm = LayoutLMv2ForTokenClassification.from_pretrained(\"sibrun/receiptlayoutlm\", use_auth_token=True)\n",
    "model_xlm.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simon/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py:772: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(\n",
      "/Users/simon/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/transformers/models/layoutlmv2/modeling_layoutlmv2.py:782: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "path_to_image = \"../data/X00016469612.jpg\"\n",
    "image = Image.open(path_to_image)\n",
    "features = feature_extractor_xlm(image, return_tensors=\"pt\")\n",
    "tokenizer_output = tokenizer_xlm(\n",
    "        text=features['words'],\n",
    "        boxes=features['boxes'],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt')\n",
    "model_input = tokenizer_output\n",
    "model_input['image'] = features['pixel_values']\n",
    "model_output = model_xlm(**model_input)\n",
    "predictions = np.argmax(model_output.logits.detach().tolist(), axis=-1)\n",
    "tokens = [tokenizer_xlm.convert_ids_to_tokens(ids) for ids in tokenizer_output['input_ids'].tolist()]\n",
    "ner_labels = [(ids_to_labels[p] for p in prediction) for prediction in predictions]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [62]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtokenizer_xlm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_tokens_to_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/transformers/models/layoutxlm/tokenization_layoutxlm.py:323\u001B[0m, in \u001B[0;36mLayoutXLMTokenizer.convert_tokens_to_string\u001B[0;34m(self, tokens)\u001B[0m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert_tokens_to_string\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokens):\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;124;03m\"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 323\u001B[0m     out_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mreplace(SPIECE_UNDERLINE, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m out_string\n",
      "\u001B[0;31mTypeError\u001B[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "tokenizer_xlm.convert_tokens_to_string(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}