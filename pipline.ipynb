{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "!sudo apt-get install git-lfs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install git-lfs\n",
    "!pip install torch torchvision\n",
    "!pip install datasets transformers\n",
    "!pip install Pillow\n",
    "!pip install pytesseract\n",
    "!pip install sentencepiece\n",
    "!pip install wandb\n",
    "#!pip install tensorflow tensorboard\n",
    "#!pip install tensorboardX\n",
    "!pip install seqeval\n",
    "!pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "path_to_home = \"../\"\n",
    "#path_to_home = \"./drive/MyDrive/receiptlayoutlm/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hf_repo = Repository(path_to_home + \"hugging_face_repo\", clone_from=\"https://huggingface.co/sibrun/receiptlayoutlm\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hf_repo = Repository(path_to_home + \"hugging_face_repo\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hf_repo.git_pull()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LayoutLMv2Tokenizer'. \n",
      "The class this function is called from is 'LayoutXLMTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv2FeatureExtractor, LayoutXLMTokenizer, LayoutXLMProcessor\n",
    "from transformers import LayoutLMv2ForTokenClassification\n",
    "\n",
    "feature_extractor_xlm = LayoutLMv2FeatureExtractor()\n",
    "tokenizer_xlm = LayoutXLMTokenizer.from_pretrained(\"microsoft/layoutxlm-base\")\n",
    "processor_xlm = LayoutXLMProcessor(feature_extractor_xlm, tokenizer_xlm)\n",
    "model_xlm = LayoutLMv2ForTokenClassification.from_pretrained(\"sibrun/receiptlayoutlm\", use_auth_token=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe_ner_xlm = pipeline('ner', model=model_xlm, tokenizer=tokenizer_xlm, feature_extractor=feature_extractor_xlm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "pipe_ner_xlm.save_pretrained(path_to_home + \"models/ner_pipe_xlm\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "path_to_image = \"../data/X00016469612.jpg\"\n",
    "image = Image.open(path_to_image)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At least one input is required.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [22]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mpipe_ner_xlm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:185\u001B[0m, in \u001B[0;36mTokenClassificationPipeline.__call__\u001B[0;34m(self, inputs, **kwargs)\u001B[0m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs: Union[\u001B[38;5;28mstr\u001B[39m, List[\u001B[38;5;28mstr\u001B[39m]], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;124;03m    Classify each token of the text(s) given as inputs.\u001B[39;00m\n\u001B[1;32m    163\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;124;03m          exists if the offsets are available within the tokenizer\u001B[39;00m\n\u001B[1;32m    183\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 185\u001B[0m     _inputs, offset_mapping \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args_parser\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m offset_mapping:\n\u001B[1;32m    187\u001B[0m         kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moffset_mapping\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m offset_mapping\n",
      "File \u001B[0;32m~/PycharmProjects/receiptlayoutlm/venv/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:35\u001B[0m, in \u001B[0;36mTokenClassificationArgumentHandler.__call__\u001B[0;34m(self, inputs, **kwargs)\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inputs, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 35\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAt least one input is required.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     37\u001B[0m offset_mapping \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moffset_mapping\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m offset_mapping:\n",
      "\u001B[0;31mValueError\u001B[0m: At least one input is required."
     ]
    }
   ],
   "source": [
    "pipe_ner_xlm(image)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TokenClassificationPipeline' object has no attribute 'push_to_hub'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [16]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mpipe_ner_xlm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpush_to_hub\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msibrun/ner_pipe_xlm\u001B[39m\u001B[38;5;124m\"\u001B[39m, use_auth_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'TokenClassificationPipeline' object has no attribute 'push_to_hub'"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}